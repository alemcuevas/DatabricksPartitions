{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40bbc8c4-de13-4a2b-b445-8b5ba0675aa5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paso 1: Cargar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09000517-d1e4-4e8b-97e8-e9799a02247c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Montar el contenedor de Azure Data Lake Storage (ADLS)\n",
    "dbutils.fs.mount(\n",
    "    source = \"\",\n",
    "    mount_point = \"/mnt/\",\n",
    "    extra_configs = {\"\": \"\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110b626b-0f20-4885-9151-c629585e5ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------+------------+-----+--------+------------+\n|_c0|      Date|Customer_ID|Transaction_ID|SKU_Category|  SKU|Quantity|Sales_Amount|\n+---+----------+-----------+--------------+------------+-----+--------+------------+\n|  1|2016-01-02|       2547|             1|         X52|0EM7L|     1.0|        3.13|\n|  2|2016-01-02|        822|             2|         2ML|68BRQ|     1.0|        5.46|\n|  3|2016-01-02|       3686|             3|         0H2|CZUZX|     1.0|        6.35|\n|  4|2016-01-02|       3719|             4|         0H2|549KK|     1.0|        5.59|\n|  5|2016-01-02|       9200|             5|         0H2|K8EHH|     1.0|        6.88|\n+---+----------+-----------+--------------+------------+-----+--------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar el dataset desde Azure Data Lake Storage\n",
    "df = spark.read.csv(\"/mnt/datalake/retail_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89735632-e924-45a8-92e6-025f1d209419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Customer_ID: integer (nullable = true)\n |-- Transaction_ID: integer (nullable = true)\n |-- SKU_Category: string (nullable = true)\n |-- SKU: string (nullable = true)\n |-- Quantity: double (nullable = true)\n |-- Sales_Amount: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Mostrar esquema inicial del dataset\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db28ff86-9ff3-416b-9af5-7db4585a2425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paso 2: Limpieza de Datos\n",
    "2.1. Identificar columnas con todos los valores nulos\n",
    "\n",
    "Este paso identifica y elimina columnas que contienen únicamente valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214cb0bb-48cd-4674-8f5c-d3368564f454",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Customer_ID: integer (nullable = true)\n |-- Transaction_ID: integer (nullable = true)\n |-- SKU_Category: string (nullable = true)\n |-- SKU: string (nullable = true)\n |-- Quantity: double (nullable = true)\n |-- Sales_Amount: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "\n",
    "# Obtener el total de filas en el dataset\n",
    "total_rows = df.count()\n",
    "\n",
    "# Contar los valores nulos por columna en una sola ejecución\n",
    "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "\n",
    "# Identificar columnas con todos los valores nulos\n",
    "all_null_columns = [c for c, null_count in zip(df.columns, null_counts) if null_count == total_rows]\n",
    "\n",
    "# Eliminar columnas completamente nulas\n",
    "df_cleaned = df.drop(*all_null_columns)\n",
    "\n",
    "# Mostrar las columnas restantes\n",
    "df_cleaned.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67cb65c-aafe-43b2-8038-c1df28442403",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2.2. Conversión de tipos de datos erróneos\n",
    "\n",
    "Verificamos y aseguramos que las columnas con tipos de datos numéricos estén correctamente casteadas. Según tu esquema, ya tenemos los tipos correctos (integer, double, date), pero este código verifica y ajusta los datos si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffa2d65-9cf7-4897-9038-9e982a4a73bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Customer_ID: integer (nullable = true)\n |-- Transaction_ID: integer (nullable = true)\n |-- SKU_Category: string (nullable = true)\n |-- SKU: string (nullable = true)\n |-- Quantity: double (nullable = true)\n |-- Sales_Amount: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Aseguramos la conversión correcta de tipos en columnas numéricas y de fecha\n",
    "df_cleaned = df_cleaned.withColumn(\"Customer_ID\", col(\"Customer_ID\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"Transaction_ID\", col(\"Transaction_ID\").cast(\"integer\")) \\\n",
    "                       .withColumn(\"Quantity\", col(\"Quantity\").cast(\"double\")) \\\n",
    "                       .withColumn(\"Sales_Amount\", col(\"Sales_Amount\").cast(\"double\"))\n",
    "\n",
    "# Mostrar el esquema actualizado\n",
    "df_cleaned.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "302f8904-4cfc-4dc9-a24c-0c2c10f32ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2.3. Llenar o eliminar valores nulos según contexto\n",
    "\n",
    "Llenamos valores nulos en columnas clave o eliminamos filas donde faltan datos críticos, por ejemplo, transacciones sin Sales_Amount o Quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587df3d3-17c2-4401-a031-4567e09c5475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+\n|summary|              _c0|       Customer_ID|    Transaction_ID|      SKU_Category|               SKU|          Quantity|      Sales_Amount|\n+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+\n|  count|           131706|            131706|            131706|            131706|            131706|            131706|            131706|\n|   mean|          65853.5|12386.450366725889|32389.604186597422| 399.9587628865979| 52972.52046783626|1.4853114436699932|11.981524152278366|\n| stddev|38020.39161423775| 6086.447551524349|18709.901237886686|220.36654679976633|31885.645096065182| 3.872667435765362|  19.3596994942029|\n|    min|                1|                 1|                 1|               01F|             00GVC|              0.01|              0.02|\n|    max|           131706|             22625|             64682|               ZYU|             ZZX6K|             400.0|            707.73|\n+-------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Rellenar nulos con valores por defecto en columnas críticas si es necesario\n",
    "df_cleaned = df_cleaned.fillna({'Quantity': 0, 'Sales_Amount': 0})\n",
    "\n",
    "# Eliminar filas con demasiados nulos si es necesario\n",
    "df_cleaned = df_cleaned.na.drop(subset=[\"Customer_ID\", \"Transaction_ID\"])\n",
    "\n",
    "# Mostrar un resumen después de la limpieza\n",
    "df_cleaned.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c2a264b-c919-4080-8d62-077de5ed58e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paso 3: Identificación y Aplicación de Particionamiento\n",
    "\n",
    "3.1. Evaluar el tamaño del dataset y decidir particionamiento\n",
    "\n",
    "Identificamos las columnas más adecuadas para particionar según la cardinalidad y el uso en consultas. En este caso, se podría particionar por SKU_Category, Customer_ID, y Date para optimizar las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd4cef4-f34b-439e-986a-bf72a67f99c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+-----+\n|Customer_ID|SKU_Category|      Date|count|\n+-----------+------------+----------+-----+\n|       2237|         LDZ|2016-01-04|    1|\n|        754|         TW8|2016-01-04|    1|\n|       1082|         1VL|2016-01-04|    1|\n|        853|         BZU|2016-01-07|    1|\n|       7223|         R6E|2016-01-07|    1|\n|       3827|         JPI|2016-01-08|    1|\n|       3957|         P42|2016-01-08|    1|\n|       2436|         P42|2016-01-08|    1|\n|       5253|         29A|2016-01-08|    1|\n|       3719|         U5F|2016-01-11|    1|\n|       3719|         0H2|2016-01-11|    1|\n|       2811|         IEV|2016-01-11|    1|\n|       2402|         IEV|2016-01-11|    1|\n|        199|         U5F|2016-01-11|    1|\n|       7060|         MOE|2016-01-11|    1|\n|       1591|         1L6|2016-01-12|    1|\n|         70|         A38|2016-01-13|    1|\n|       3182|         LPF|2016-01-13|    1|\n|       9095|         R6E|2016-01-13|    1|\n|       5612|         RU6|2016-01-14|    1|\n+-----------+------------+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:844)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:843)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:460)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:577)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:57)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:57)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:559)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:818)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:844)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:843)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:898)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:691)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:527)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:631)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:651)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:626)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:536)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:496)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluamos la cardinalidad de algunas columnas para decidir el particionamiento\n",
    "df_cleaned.select(\"Customer_ID\", \"SKU_Category\", \"Date\").groupBy(\"Customer_ID\", \"SKU_Category\", \"Date\").count().show()\n",
    "\n",
    "# Aplicar particionamiento basado en columnas de alta cardinalidad\n",
    "df_partitioned = df_cleaned.repartition(\"Customer_ID\", \"SKU_Category\", \"Date\")\n",
    "\n",
    "# Guardar el dataset particionado en formato Parquet\n",
    "df_partitioned.write.partitionBy(\"Customer_ID\", \"SKU_Category\", \"Date\").mode(\"overwrite\").parquet(\"/mnt/datalake/partitioned_data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44643b22-d430-42f2-9ecd-02b577e12c0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.2. Optimización con Adaptive Query Execution (AQE)\n",
    "\n",
    "Habilitamos AQE y Dynamic Partition Pruning (DPP) para optimizar las consultas en las tablas particionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2287614-aa68-4626-b4e7-c203b32c269b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Activar Adaptive Query Execution (AQE)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Activar Dynamic Partition Pruning (DPP)\n",
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "906f8cd9-53f9-4d2e-b6ed-f01fd81fce5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paso 4: Monitoreo del Rendimiento dentro del Código\n",
    "\n",
    "4.1. Medir tiempo de ejecución\n",
    "\n",
    "Se mide el tiempo de ejecución de consultas o transformaciones clave dentro del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e5ca63-52a5-45df-9148-ac3791812af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n|SKU_Category| sum(Sales_Amount)|\n+------------+------------------+\n|         0H2|              7.29|\n|         2ML|13.149999999999999|\n+------------+------------------+\n\nTiempo de ejecución: 1.3380613327026367 segundos\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Medir el tiempo de ejecución de una operación de consulta\n",
    "start_time = time.time()\n",
    "\n",
    "# Realizar una consulta en el dataset particionado\n",
    "df_partitioned.filter(\"Customer_ID = 12345\").groupBy(\"SKU_Category\").sum(\"Sales_Amount\").show()\n",
    "\n",
    "# Mostrar el tiempo de ejecución\n",
    "print(f\"Tiempo de ejecución: {time.time() - start_time} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6440243a-7424-4e94-bd60-9088c11f0a9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.2. Monitorear el uso de recursos: CPU y memoria\n",
    "\n",
    "Monitoreamos el uso de recursos directamente en el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bfd359-33b2-4135-9ed5-0cd24581790b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID del ejecutor: 10.139.64.4:45095, Memoria usada: 6558842880, Memoria total: 6558842880\nID del ejecutor: 10.139.64.5:33779, Memoria usada: 6558842880, Memoria total: 6558842880\nID del ejecutor: 10.139.64.6:35641, Memoria usada: 9431207116, Memoria total: 9431188837\n"
     ]
    }
   ],
   "source": [
    "# Obtener el número de particiones procesadas y tareas completadas\n",
    "executor_memory_status = sc._jsc.sc().getExecutorMemoryStatus().toString()\n",
    "executor_memory_status = executor_memory_status.replace(\"Map(\", \"\").replace(\")\", \"\")\n",
    "executor_memory_status = dict(\n",
    "    item.split(\" -> \") for item in executor_memory_status.split(\", \")\n",
    ")\n",
    "\n",
    "for executor_id, memory in executor_memory_status.items():\n",
    "    memory_used, memory_total = map(int, memory.strip(\"()\").split(\",\"))\n",
    "    print(f\"ID del ejecutor: {executor_id}, Memoria usada: {memory_used}, Memoria total: {memory_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd21a4d-bf47-4d8a-ae15-228a3437fdb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4.3. Resumen de plan de ejecución\n",
    "\n",
    "Para verificar cómo se optimiza el trabajo, utilizamos el método explain() para obtener el plan físico y lógico de las operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5768eae0-d39d-4b3a-8273-14d24466b67f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['SKU_Category], ['SKU_Category, unresolvedalias(sum(Sales_Amount#368))]\n+- Filter (Customer_ID#323 = 12345)\n   +- RepartitionByExpression [Customer_ID#323, SKU_Category#98, Date#95]\n      +- Filter atleastnnonnulls(2, Customer_ID#323, Transaction_ID#332)\n         +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, coalesce(nanvl(Quantity#341, cast(null as double)), cast(0 as double)) AS Quantity#367, coalesce(nanvl(Sales_Amount#350, cast(null as double)), cast(0 as double)) AS Sales_Amount#368]\n            +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, Quantity#341, cast(Sales_Amount#101 as double) AS Sales_Amount#350]\n               +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, cast(Quantity#100 as double) AS Quantity#341, Sales_Amount#101]\n                  +- Project [_c0#94, Date#95, Customer_ID#323, cast(Transaction_ID#97 as int) AS Transaction_ID#332, SKU_Category#98, SKU#99, Quantity#100, Sales_Amount#101]\n                     +- Project [_c0#94, Date#95, cast(Customer_ID#96 as int) AS Customer_ID#323, Transaction_ID#97, SKU_Category#98, SKU#99, Quantity#100, Sales_Amount#101]\n                        +- Relation [_c0#94,Date#95,Customer_ID#96,Transaction_ID#97,SKU_Category#98,SKU#99,Quantity#100,Sales_Amount#101] csv\n\n== Analyzed Logical Plan ==\nSKU_Category: string, sum(Sales_Amount): double\nAggregate [SKU_Category#98], [SKU_Category#98, sum(Sales_Amount#368) AS sum(Sales_Amount)#1441]\n+- Filter (Customer_ID#323 = 12345)\n   +- RepartitionByExpression [Customer_ID#323, SKU_Category#98, Date#95]\n      +- Filter atleastnnonnulls(2, Customer_ID#323, Transaction_ID#332)\n         +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, coalesce(nanvl(Quantity#341, cast(null as double)), cast(0 as double)) AS Quantity#367, coalesce(nanvl(Sales_Amount#350, cast(null as double)), cast(0 as double)) AS Sales_Amount#368]\n            +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, Quantity#341, cast(Sales_Amount#101 as double) AS Sales_Amount#350]\n               +- Project [_c0#94, Date#95, Customer_ID#323, Transaction_ID#332, SKU_Category#98, SKU#99, cast(Quantity#100 as double) AS Quantity#341, Sales_Amount#101]\n                  +- Project [_c0#94, Date#95, Customer_ID#323, cast(Transaction_ID#97 as int) AS Transaction_ID#332, SKU_Category#98, SKU#99, Quantity#100, Sales_Amount#101]\n                     +- Project [_c0#94, Date#95, cast(Customer_ID#96 as int) AS Customer_ID#323, Transaction_ID#97, SKU_Category#98, SKU#99, Quantity#100, Sales_Amount#101]\n                        +- Relation [_c0#94,Date#95,Customer_ID#96,Transaction_ID#97,SKU_Category#98,SKU#99,Quantity#100,Sales_Amount#101] csv\n\n== Optimized Logical Plan ==\nAggregate [SKU_Category#98], [SKU_Category#98, sum(Sales_Amount#368) AS sum(Sales_Amount)#1441]\n+- Project [SKU_Category#98, Sales_Amount#368]\n   +- RepartitionByExpression [Customer_ID#96, SKU_Category#98, Date#95]\n      +- Project [Date#95, Customer_ID#96, SKU_Category#98, coalesce(nanvl(Sales_Amount#101, null), 0.0) AS Sales_Amount#368]\n         +- Filter ((isnotnull(Customer_ID#96) AND atleastnnonnulls(2, Customer_ID#96, Transaction_ID#97)) AND (Customer_ID#96 = 12345))\n            +- Relation [_c0#94,Date#95,Customer_ID#96,Transaction_ID#97,SKU_Category#98,SKU#99,Quantity#100,Sales_Amount#101] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonGroupingAgg(limit=None, keys=[SKU_Category#98], functions=[finalmerge_sum(merge sum#1445) AS sum(Sales_Amount#368)#1440], output=[SKU_Category#98, sum(Sales_Amount)#1441])\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage\n               +- PhotonShuffleExchangeSink hashpartitioning(SKU_Category#98, 200)\n                  +- PhotonGroupingAgg(limit=None, keys=[SKU_Category#98], functions=[partial_sum(Sales_Amount#368) AS sum#1445], output=[SKU_Category#98, sum#1445])\n                     +- PhotonProject [SKU_Category#98, Sales_Amount#368]\n                        +- PhotonShuffleExchangeSource\n                           +- PhotonShuffleMapStage\n                              +- PhotonShuffleExchangeSink hashpartitioning(Customer_ID#96, SKU_Category#98, Date#95, 200)\n                                 +- PhotonProject [Date#95, Customer_ID#96, SKU_Category#98, coalesce(nanvl(Sales_Amount#101, null), 0.0) AS Sales_Amount#368]\n                                    +- PhotonFilter ((isnotnull(Customer_ID#96) AND atleastnnonnulls(2, Customer_ID#96, Transaction_ID#97)) AND (Customer_ID#96 = 12345))\n                                       +- PhotonRowToColumnar\n                                          +- FileScan csv [Date#95,Customer_ID#96,Transaction_ID#97,SKU_Category#98,Sales_Amount#101] Batched: false, DataFilters: [isnotnull(Customer_ID#96), atleastnnonnulls(2, Customer_ID#96, Transaction_ID#97), (Customer_ID#..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/mnt/datalake/retail_dataset.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Customer_ID), EqualTo(Customer_ID,12345)], ReadSchema: struct<Date:date,Customer_ID:int,Transaction_ID:int,SKU_Category:string,Sales_Amount:double>\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el plan de ejecución para una consulta compleja\n",
    "df_partitioned.filter(\"Customer_ID = 12345\").groupBy(\"SKU_Category\").sum(\"Sales_Amount\").explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d907bf4c-a40f-485a-a206-fee635058fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plan Lógico (Parsed Logical Plan)\n",
    "\n",
    "**'Aggregate ['SKU_Category]:** La consulta está agregando los datos (sumando Sales_Amount) agrupados por la columna SKU_Category.\n",
    "\n",
    "**'Filter (Customer_ID = 12345):** Un filtro se aplica a la columna Customer_ID para seleccionar las filas donde Customer_ID es igual a 12345.\n",
    "\n",
    "**'RepartitionByExpression [Customer_ID, SKU_Category, Date]:** Los datos son repartidos por las columnas Customer_ID, SKU_Category, y Date para distribuir las filas a través de diferentes particiones, lo que ayuda a paralelizar el procesamiento.\n",
    "\n",
    "**'Project y 'Coalesce:** Se proyectan y ajustan varias columnas, como Quantity y Sales_Amount, donde se están utilizando funciones como coalesce y nanvl para reemplazar valores nulos con 0.\n",
    "\n",
    "**'Relation csv:** Los datos se están leyendo de un archivo CSV con las columnas especificadas: Date, Customer_ID, Transaction_ID, SKU_Category, SKU, Quantity, y Sales_Amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1dfdb90-81c0-4558-8d56-43bfa56f1699",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plan Físico (Physical Plan)\n",
    "\n",
    "**PhotonGroupingAgg:** La agregación ocurre en dos etapas. Primero, Spark realiza una agregación parcial (partial_sum) para cada SKU_Category y luego hace un \"merge\" de los resultados parciales para obtener la suma total de Sales_Amount.\n",
    "\n",
    "**PhotonShuffleExchange:** Spark usa un \"shuffle\" para redistribuir los datos por SKU_Category y Customer_ID en 200 particiones, lo que permite procesar los datos en paralelo a lo largo del clúster.\n",
    "\n",
    "**PhotonProject y PhotonFilter:** Se proyectan las columnas necesarias (SKU_Category, Sales_Amount) y se aplica el filtro para Customer_ID = 12345 lo más temprano posible.\n",
    "\n",
    "**FileScan csv:** Finalmente, los datos se leen del archivo CSV con las columnas especificadas, aplicando filtros de datos como isnotnull(Customer_ID) y Customer_ID = 12345 antes de procesar las filas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e157622-67a9-42ea-a065-b13d4c659c9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Optimizaciones a partir del Plan Lógico\n",
    "\n",
    "Evitar particionamiento innecesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535ac492-a830-4d78-b0a8-b652954544ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_partitioned = df_cleaned.filter(\"Customer_ID = 12345\").groupBy(\"SKU_Category\").sum(\"Sales_Amount\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebbe2bb-3a9a-4e9f-8ebc-19c638651360",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convertir CSV a Parquet o a Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "063924cf-f7bd-4a16-a33e-8777163097c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"/mnt/datalake/parquet_dataset/\")\n",
    "\n",
    "# Then, read from Parquet:\n",
    "df = spark.read.parquet(\"/mnt/datalake/parquet_dataset/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00a787d5-932e-4a7c-a891-3b0a722bdf04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Uso de cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66667e7a-059b-4d13-9b33-44d5de5d890f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n|SKU_Category| sum(Sales_Amount)|\n+------------+------------------+\n|         0H2|              7.29|\n|         2ML|13.149999999999999|\n+------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.cache()\n",
    "df_cleaned.filter(\"Customer_ID = 12345\").groupBy(\"SKU_Category\").sum(\"Sales_Amount\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c181a1-528c-47de-9938-3b48125d9eff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Paso 5: Guardar y Finalizar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c98e953-c613-4dc1-a13f-4f38fe1a1be6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar el dataset final en formato Parquet\n",
    "df_cleaned.write.parquet(\"/mnt/datalake/final_cleaned_data/\", mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
